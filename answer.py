"""
4-step answer generation
"""

import os
from typing import List

from intent import IntentResult
from planner import FetchPlan
from summarizer import SourceSummary
from config import (
    QUESTION_TYPE_BUY_RECOMMENDATION,
    QUESTION_TYPE_PRICE_STATUS,
    QUESTION_TYPE_PUBLIC_OPINION,
    QUESTION_TYPE_NEWS_DISCLOSURE,
    QUESTION_TYPE_OTHER,
    get_env
)


def _get_intent_description(question_type: str) -> str:
    """
    Get human-readable description of question type
    Args:
        question_type: Question type code
    Returns:
        Description string
    """
    descriptions = {
        QUESTION_TYPE_BUY_RECOMMENDATION: "ë§¤ìˆ˜/íˆ¬ì íŒë‹¨ì— ëŒ€í•œ ì •ë³´ë¥¼ ì›í•˜ì‹œëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤",
        QUESTION_TYPE_PRICE_STATUS: "í˜„ì¬ ì‹œì„¸ ë° ê°€ê²© ì •ë³´ë¥¼ í™•ì¸í•˜ê³ ì í•˜ì‹œëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤",
        QUESTION_TYPE_PUBLIC_OPINION: "ë‹¤ë¥¸ íˆ¬ììë“¤ì˜ ì˜ê²¬ê³¼ ë°˜ì‘ì„ ì•Œê³  ì‹¶ìœ¼ì‹  ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤",
        QUESTION_TYPE_NEWS_DISCLOSURE: "ìµœê·¼ ë‰´ìŠ¤ ë° ê³µì‹œ ë‚´ìš©ì„ í™•ì¸í•˜ê³ ì í•˜ì‹œëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤",
        QUESTION_TYPE_OTHER: "ì¢…ëª©ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì •ë³´ë¥¼ ì›í•˜ì‹œëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤"
    }
    return descriptions.get(question_type, "ì§ˆë¬¸ ì˜ë„ë¥¼ íŒŒì•…í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤")


def _generate_final_answer_basic(
    intent: IntentResult,
    summaries: List[SourceSummary]
) -> str:
    """
    Generate final answer using template (basic mode)
    Args:
        intent: Intent analysis result
        summaries: Source summaries
    Returns:
        Final answer text
    """
    question_type = intent.question_type

    # Extract key data from summaries
    price_data = None
    news_data = None
    talks_data = None

    for summary in summaries:
        # Match price data from any source (HTML, API)
        if "ì‹œì„¸ ì •ë³´" in summary.source_type:
            price_data = summary
        elif summary.source_type == "ë‰´ìŠ¤":
            news_data = summary
        elif summary.source_type == "í† ë¡ /ì˜ê²¬":
            talks_data = summary

    # Generate answer based on question type
    if question_type == QUESTION_TYPE_BUY_RECOMMENDATION:
        answer = "**[ë‹¤ìŒ ê¸ˆìœµ ë°ì´í„° ê¸°ë°˜ ë¶„ì„]**\n\n"

        if price_data:
            answer += f"**í˜„ì¬ ìƒíƒœ:**\n{price_data.evidence_snippet}\n\n"

        if news_data:
            answer += f"**ìµœê·¼ ë‰´ìŠ¤:**\n{news_data.evidence_snippet}\n\n"

        if talks_data:
            answer += f"**íˆ¬ìì ì˜ê²¬:**\n{talks_data.evidence_snippet}\n\n"

        answer += "**ì²´í¬í¬ì¸íŠ¸:**\n"
        answer += "- ìœ„ ì •ë³´ëŠ” ë‹¤ìŒ ê¸ˆìœµì—ì„œ ìˆ˜ì§‘í•œ í˜„ì¬ ì‹œì  ë°ì´í„°ì…ë‹ˆë‹¤\n"
        answer += "- íˆ¬ì ê²°ì •ì€ ë³¸ì¸ì˜ íˆ¬ì ì„±í–¥ê³¼ ì¬ë¬´ ìƒí™©ì„ ê³ ë ¤í•˜ì—¬ ì‹ ì¤‘íˆ ê²°ì •í•˜ì„¸ìš”\n"
        answer += "- ì¶”ê°€ë¡œ ê¸°ì—… ì¬ë¬´ì œí‘œ, ì—…ì¢… ë™í–¥ ë“±ì„ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤\n"

    elif question_type == QUESTION_TYPE_PRICE_STATUS:
        answer = "**[í˜„ì¬ ì‹œì„¸ ì •ë³´]**\n\n"

        if price_data:
            answer += f"{price_data.evidence_snippet}\n\n"
        else:
            answer += "ì‹œì„¸ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"

        answer += "*â€» ì‹¤ì‹œê°„ ë°ì´í„°ê°€ ì•„ë‹ ìˆ˜ ìˆìœ¼ë©°, ì •í™•í•œ ì •ë³´ëŠ” ë‹¤ìŒ ê¸ˆìœµ ì‚¬ì´íŠ¸ì—ì„œ í™•ì¸í•˜ì„¸ìš”.*\n"

    elif question_type == QUESTION_TYPE_PUBLIC_OPINION:
        answer = "**[íˆ¬ìì ì˜ê²¬ ìš”ì•½]**\n\n"

        if talks_data:
            answer += f"{talks_data.evidence_snippet}\n\n"
        else:
            answer += "ìµœê·¼ ì˜ê²¬ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"

        if price_data:
            answer += f"**ì°¸ê³  - í˜„ì¬ ì‹œì„¸:**\n{price_data.evidence_snippet}\n\n"

        answer += "*â€» ê°œì¸ ì˜ê²¬ì´ë¯€ë¡œ ì°¸ê³ ë§Œ í•˜ì‹œê³ , íˆ¬ì íŒë‹¨ì€ ë³¸ì¸ì˜ ì±…ì„í•˜ì— í•˜ì„¸ìš”.*\n"

    elif question_type == QUESTION_TYPE_NEWS_DISCLOSURE:
        answer = "**[ìµœê·¼ ë‰´ìŠ¤ ë° ê³µì‹œ]**\n\n"

        if news_data:
            answer += f"**ë‰´ìŠ¤:**\n{news_data.evidence_snippet}\n\n"

        found_disclosure = False
        for summary in summaries:
            if summary.source_type == "ê³µì‹œ":
                answer += f"**ê³µì‹œ:**\n{summary.evidence_snippet}\n\n"
                found_disclosure = True
                break

        if not news_data and not found_disclosure:
            answer += "ìµœê·¼ ë‰´ìŠ¤ë‚˜ ê³µì‹œë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"

    else:
        answer = "**[ì¢…ëª© ì •ë³´]**\n\n"

        if price_data:
            answer += f"{price_data.evidence_snippet}\n\n"
        else:
            answer += "ì¢…ëª© ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"

    return answer


def _generate_final_answer_llm(
    intent: IntentResult,
    summaries: List[SourceSummary],
    chat_history: List = None
) -> str:
    """
    Generate final answer using LLM (optional mode)
    Args:
        intent: Intent analysis result
        summaries: Source summaries
        chat_history: Previous chat messages for context (optional)
    Returns:
        Final answer text
    """
    try:
        api_key = get_env('ANTHROPIC_API_KEY') or get_env('OPENAI_API_KEY')
        if not api_key:
            return _generate_final_answer_basic(intent, summaries)

        # Prepare evidence snippets
        evidence = "\n\n".join([
            f"[{summary.source_type}]\n{summary.evidence_snippet}"
            for summary in summaries
        ])

        # Prepare chat history context
        history_context = ""
        if chat_history and len(chat_history) > 0:
            history_context = "\n**ì´ì „ ëŒ€í™” ë‚´ìš©:**\n"
            for msg in chat_history[-6:]:  # Last 3 exchanges
                role = "ì‚¬ìš©ì" if msg.role == "user" else "ì±—ë´‡"
                history_context += f"{role}: {msg.content[:200]}...\n"
            history_context += "\n"

        prompt_text = f"""ë‹¹ì‹ ì€ ë‹¤ìŒ ê¸ˆìœµ(finance.daum.net) ë°ì´í„° ê¸°ë°˜ íˆ¬ì ì •ë³´ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.

{history_context}**í•µì‹¬ ì›ì¹™:**
1. ì•„ë˜ ì œê³µëœ ë‹¤ìŒ ê¸ˆìœµ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
2. ì™¸ë¶€ ì§€ì‹, í•™ìŠµëœ ì •ë³´, ì¸í„°ë„· ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
3. ì œê³µëœ ë°ì´í„°ì— ì—†ëŠ” ë‚´ìš©ì€ "í™•ì¸ë˜ì§€ ì•ŠìŒ" ë˜ëŠ” "ë°ì´í„° ì—†ìŒ"ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”

**ì§ˆë¬¸ ì •ë³´:**
- ì§ˆë¬¸ ìœ í˜•: {intent.question_type}
- ëŒ€ìƒ ì¢…ëª©: {intent.stock_name} ({intent.stock_code})

**ë‹¤ìŒ ê¸ˆìœµì—ì„œ ìˆ˜ì§‘í•œ ë°ì´í„°:**
{evidence}

**ë‹µë³€ ì‘ì„± ì§€ì¹¨:**
ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì´ˆë³´ íˆ¬ììê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•˜ë˜, ë°˜ë“œì‹œ ì œê³µëœ ë‹¤ìŒ ê¸ˆìœµ ë°ì´í„°ë§Œ í™œìš©í•˜ì„¸ìš”.

## ğŸ“Š {intent.stock_name} ({intent.stock_code}) ë¶„ì„

### í•œ ì¤„ ìš”ì•½
(ë‹¤ìŒ ê¸ˆìœµ ë°ì´í„°ì—ì„œ í™•ì¸ëœ í•µì‹¬ ë‚´ìš©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½)

### ë‹¤ìŒ ê¸ˆìœµ ë°ì´í„° ê¸°ë°˜ ë¶„ì„

**í˜„ì¬ ìƒí™©:**
(ì œê³µëœ ê·¼ê±° ìŠ¤ë‹ˆí«ì˜ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í˜„ì¬ ìƒíƒœë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…)
- ì‹œì„¸ ì •ë³´ê°€ ìˆë‹¤ë©´: í˜„ì¬ê°€, ë“±ë½ë¥ , ê±°ë˜ëŸ‰ ë“±ì„ ëª…í™•íˆ ì œì‹œ
- ë‰´ìŠ¤ ì •ë³´ê°€ ìˆë‹¤ë©´: ì£¼ìš” ë‰´ìŠ¤ ì œëª©ê³¼ ë‚´ìš©ì„ ìš”ì•½
- ì˜ê²¬ ì •ë³´ê°€ ìˆë‹¤ë©´: íˆ¬ììë“¤ì˜ ì˜ê²¬ ë™í–¥ì„ ê°ê´€ì ìœ¼ë¡œ ì „ë‹¬

**ì²´í¬í¬ì¸íŠ¸:**
- ìœ„ ì •ë³´ëŠ” ë‹¤ìŒ ê¸ˆìœµì—ì„œ ìˆ˜ì§‘í•œ í˜„ì¬ ì‹œì  ë°ì´í„°ì…ë‹ˆë‹¤
- íˆ¬ì ê²°ì •ì€ ë³¸ì¸ì˜ íˆ¬ì ì„±í–¥ê³¼ ì¬ë¬´ ìƒí™©ì„ ê³ ë ¤í•˜ì—¬ ì‹ ì¤‘íˆ ê²°ì •í•˜ì„¸ìš”
- ì¶”ê°€ë¡œ ê¸°ì—… ì¬ë¬´ì œí‘œ, ì—…ì¢… ë™í–¥ ë“±ì„ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤

**ì‘ì„± ì‹œ ì£¼ì˜ì‚¬í•­:**
âŒ ê¸ˆì§€:
- "~í•  ê²ƒì…ë‹ˆë‹¤", "~í•  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤" ê°™ì€ ì˜ˆì¸¡ì„± í‘œí˜„
- "ë§¤ìˆ˜í•˜ì„¸ìš”", "íˆ¬ìí•˜ì„¸ìš”" ê°™ì€ ê¶Œìœ  í‘œí˜„
- ì œê³µë˜ì§€ ì•Šì€ ë°ì´í„°ë‚˜ ì™¸ë¶€ ì§€ì‹ ì–¸ê¸‰

âœ… ê¶Œì¥:
- "ë‹¤ìŒ ê¸ˆìœµ ë°ì´í„°ì— ë”°ë¥´ë©´ ~ì…ë‹ˆë‹¤"
- "í˜„ì¬ ì‹œì  ê¸°ì¤€ ~ë¡œ í™•ì¸ë©ë‹ˆë‹¤"
- "~ë¥¼ ì¶”ê°€ë¡œ í™•ì¸í•´ë³´ì‹œëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤"
- ë°ì´í„°ì— ì—†ëŠ” ë‚´ìš©ì€ ëª…í™•íˆ "í™•ì¸ë˜ì§€ ì•ŠìŒ"ìœ¼ë¡œ í‘œì‹œ

ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:"""

        # Use Anthropic Claude if available
        if get_env('ANTHROPIC_API_KEY'):
            import anthropic
            from config import LLM_MODEL_ANTHROPIC, LLM_MAX_TOKENS, LLM_TEMPERATURE

            client = anthropic.Anthropic(api_key=get_env('ANTHROPIC_API_KEY'))

            message = client.messages.create(
                model=LLM_MODEL_ANTHROPIC,
                max_tokens=LLM_MAX_TOKENS,
                temperature=LLM_TEMPERATURE,
                messages=[{"role": "user", "content": prompt_text}]
            )

            return message.content[0].text

        # Use OpenAI if Anthropic is not available
        elif get_env('OPENAI_API_KEY'):
            from openai import OpenAI
            from config import LLM_MODEL_OPENAI, LLM_MAX_TOKENS, LLM_TEMPERATURE

            client = OpenAI(api_key=get_env('OPENAI_API_KEY'))

            response = client.chat.completions.create(
                model=LLM_MODEL_OPENAI,
                max_tokens=LLM_MAX_TOKENS,
                temperature=LLM_TEMPERATURE,
                messages=[{"role": "user", "content": prompt_text}]
            )

            return response.choices[0].message.content

        # Fallback to basic mode
        return _generate_final_answer_basic(intent, summaries)

    except Exception:
        return _generate_final_answer_basic(intent, summaries)


def generate_answer(
    intent: IntentResult,
    plans: List[FetchPlan],
    summaries: List[SourceSummary],
    use_llm: bool = False,
    show_details: bool = True,
    chat_history: List = None
) -> str:
    """
    Generate 4-step structured answer

    Args:
        intent: Intent analysis result
        plans: Fetch plans
        summaries: Source summaries
        use_llm: Whether to use LLM for answer generation (default: False)
        show_details: Whether to show detailed steps 1-3 (default: True)
        chat_history: Previous chat messages for context (optional)

    Returns:
        Complete answer as markdown string
    """
    output = []

    # Show detailed steps only if requested
    if show_details:
        # Step 1: Intent Analysis
        output.append("### [1] ì§ˆë¬¸ ì˜ë„ ë¶„ì„\n")
        output.append(f"- **ì§ˆë¬¸ ìœ í˜•:** {intent.question_type}")
        output.append(f"- **ëŒ€ìƒ ì¢…ëª©:** {intent.stock_name or 'í™•ì¸ ë¶ˆê°€'} ({intent.stock_code or 'í™•ì¸ ë¶ˆê°€'})")
        output.append(f"- **ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ê²ƒ:** {_get_intent_description(intent.question_type)}\n")

        # Step 2: Exploration Plan
        output.append("### [2] ë‹¤ìŒ ê¸ˆìœµ íƒìƒ‰ ê³„íš\n")
        if plans:
            for i, plan in enumerate(plans, 1):
                output.append(f"- **Plan {i}:** {plan.description}")
                output.append(f"  - URL: `{plan.url}`")
        else:
            output.append("- íƒìƒ‰ ê³„íšì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ì¢…ëª© ì •ë³´ ë¶€ì¡±)\n")
        output.append("")

        # Step 3: Scraping Results Summary
        output.append("### [3] ë‹¤ìŒ ê¸ˆìœµ ìŠ¤í¬ë© ê²°ê³¼ ìš”ì•½\n")
        if summaries:
            for i, summary in enumerate(summaries, 1):
                output.append(f"**Source {i}: {summary.source_type}**")
                output.append(f"- URL: `{summary.source_url}`")
                output.append(f"- ê·¼ê±° ìŠ¤ë‹ˆí«:\n```\n{summary.evidence_snippet}\n```\n")
        else:
            output.append("- ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n")

        # Step 4: Final Answer
        output.append("### [4] ìµœì¢… ë‹µë³€ (ì´ˆë³´ì ì¹œí™”)\n")

    # Generate final answer (always shown)
    if summaries:
        if use_llm:
            final_answer = _generate_final_answer_llm(intent, summaries, chat_history)
        else:
            final_answer = _generate_final_answer_basic(intent, summaries)
        output.append(final_answer)
    else:
        output.append("ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ìˆëŠ” ì¶©ë¶„í•œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        output.append("ì¢…ëª© ì½”ë“œë¥¼ í™•ì¸í•˜ê±°ë‚˜, ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n")

    # Reference section - ALWAYS show (even when show_details=False)
    # This ensures users can verify all data comes from finance.daum.net
    output.append("\n---")
    
    if show_details:
        output.append("### ğŸ“ ì°¸ê³ í•œ ë‹¤ìŒ ê¸ˆìœµ í˜ì´ì§€\n")
    else:
        output.append("**ğŸ“ ì°¸ê³ í•œ ë‹¤ìŒ ê¸ˆìœµ í˜ì´ì§€**\n")

    if summaries:
        # Collect unique URLs
        reference_urls = []
        seen_urls = set()

        for summary in summaries:
            url = summary.source_url
            if url and url not in seen_urls:
                seen_urls.add(url)
                reference_urls.append({
                    'type': summary.source_type,
                    'url': url
                })

        # Display as clickable links
        if reference_urls:
            for i, ref in enumerate(reference_urls[:7], 1):  # Limit to 7 references
                # Extract a friendly name from URL or use source type
                friendly_name = ref['type'] or f"ì°¸ê³  {i}"
                output.append(f"{i}. [{friendly_name}]({ref['url']})")
        else:
            output.append("- ì°¸ê³  URL ì—†ìŒ")
    else:
        output.append("- ìˆ˜ì§‘ëœ ë°ì´í„° ì—†ìŒ")

    output.append("")

    # Footer - compact version
    if show_details:
        output.append("---")
        output.append("**âš ï¸ ì£¼ì˜ì‚¬í•­:**")
        output.append("- ë³¸ ì •ë³´ëŠ” ë‹¤ìŒ ê¸ˆìœµ(finance.daum.net) ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤")
        output.append("- íˆ¬ì íŒë‹¨ ë° ê²°ê³¼ì— ëŒ€í•œ ì±…ì„ì€ íˆ¬ìì ë³¸ì¸ì—ê²Œ ìˆìŠµë‹ˆë‹¤")
        output.append("- ì‹¤ì‹œê°„ ë°ì´í„°ê°€ ì•„ë‹ ìˆ˜ ìˆìœ¼ë‹ˆ, ì •í™•í•œ ì •ë³´ëŠ” ì§ì ‘ í™•ì¸í•˜ì„¸ìš”")
    else:
        # Minimal footer for clean mode
        output.append("\n\n---")
        output.append("*ë³¸ ì •ë³´ëŠ” ë‹¤ìŒ ê¸ˆìœµ ë°ì´í„° ê¸°ë°˜ì´ë©°, íˆ¬ì íŒë‹¨ì€ ë³¸ì¸ ì±…ì„ì…ë‹ˆë‹¤*")

    return "\n".join(output)
