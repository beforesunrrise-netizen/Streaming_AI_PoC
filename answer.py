"""
4-step answer generation
"""

import os
from typing import List

from intent import IntentResult
from planner import FetchPlan
from summarizer import SourceSummary
from config import (
    QUESTION_TYPE_BUY_RECOMMENDATION,
    QUESTION_TYPE_PRICE_STATUS,
    QUESTION_TYPE_PUBLIC_OPINION,
    QUESTION_TYPE_NEWS_DISCLOSURE,
    QUESTION_TYPE_OTHER,
    get_env
)


def _get_intent_description(question_type: str) -> str:
    """
    Get human-readable description of question type
    Args:
        question_type: Question type code
    Returns:
        Description string
    """
    descriptions = {
        QUESTION_TYPE_BUY_RECOMMENDATION: "ë§¤ìˆ˜/íˆ¬ì íŒë‹¨ì— ëŒ€í•œ ì •ë³´ë¥¼ ì›í•˜ì‹œëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤",
        QUESTION_TYPE_PRICE_STATUS: "í˜„ì¬ ì‹œì„¸ ë° ê°€ê²© ì •ë³´ë¥¼ í™•ì¸í•˜ê³ ì í•˜ì‹œëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤",
        QUESTION_TYPE_PUBLIC_OPINION: "ë‹¤ë¥¸ íˆ¬ììë“¤ì˜ ì˜ê²¬ê³¼ ë°˜ì‘ì„ ì•Œê³  ì‹¶ìœ¼ì‹  ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤",
        QUESTION_TYPE_NEWS_DISCLOSURE: "ìµœê·¼ ë‰´ìŠ¤ ë° ê³µì‹œ ë‚´ìš©ì„ í™•ì¸í•˜ê³ ì í•˜ì‹œëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤",
        QUESTION_TYPE_OTHER: "ì¢…ëª©ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì •ë³´ë¥¼ ì›í•˜ì‹œëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤"
    }
    return descriptions.get(question_type, "ì§ˆë¬¸ ì˜ë„ë¥¼ íŒŒì•…í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤")


def _generate_final_answer_basic(
    intent: IntentResult,
    summaries: List[SourceSummary]
) -> str:
    """
    Generate final answer using template (basic mode)
    Args:
        intent: Intent analysis result
        summaries: Source summaries
    Returns:
        Final answer text
    """
    question_type = intent.question_type

    # Extract key data from summaries
    price_data = None
    news_data = None
    talks_data = None

    for summary in summaries:
        # Match price data from any source (HTML, API)
        if "ì‹œì„¸ ì •ë³´" in summary.source_type:
            price_data = summary
        elif summary.source_type == "ë‰´ìŠ¤":
            news_data = summary
        elif summary.source_type == "í† ë¡ /ì˜ê²¬":
            talks_data = summary

    # Generate answer based on question type
    if question_type == QUESTION_TYPE_BUY_RECOMMENDATION:
        answer = "**[ë‹¤ìŒ ê¸ˆìœµ ë°ì´í„° ê¸°ë°˜ ë¶„ì„]**\n\n"

        if price_data:
            answer += f"**í˜„ì¬ ìƒíƒœ:**\n{price_data.evidence_snippet}\n\n"

        if news_data:
            answer += f"**ìµœê·¼ ë‰´ìŠ¤:**\n{news_data.evidence_snippet}\n\n"

        if talks_data:
            answer += f"**íˆ¬ìì ì˜ê²¬:**\n{talks_data.evidence_snippet}\n\n"

        answer += "**ì²´í¬í¬ì¸íŠ¸:**\n"
        answer += "- ìœ„ ì •ë³´ëŠ” ë‹¤ìŒ ê¸ˆìœµì—ì„œ ìˆ˜ì§‘í•œ í˜„ì¬ ì‹œì  ë°ì´í„°ì…ë‹ˆë‹¤\n"
        answer += "- íˆ¬ì ê²°ì •ì€ ë³¸ì¸ì˜ íˆ¬ì ì„±í–¥ê³¼ ì¬ë¬´ ìƒí™©ì„ ê³ ë ¤í•˜ì—¬ ì‹ ì¤‘íˆ ê²°ì •í•˜ì„¸ìš”\n"
        answer += "- ì¶”ê°€ë¡œ ê¸°ì—… ì¬ë¬´ì œí‘œ, ì—…ì¢… ë™í–¥ ë“±ì„ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤\n"

    elif question_type == QUESTION_TYPE_PRICE_STATUS:
        answer = "**[í˜„ì¬ ì‹œì„¸ ì •ë³´]**\n\n"

        if price_data:
            answer += f"{price_data.evidence_snippet}\n\n"
        else:
            answer += "ì‹œì„¸ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"

        answer += "*â€» ì‹¤ì‹œê°„ ë°ì´í„°ê°€ ì•„ë‹ ìˆ˜ ìˆìœ¼ë©°, ì •í™•í•œ ì •ë³´ëŠ” ë‹¤ìŒ ê¸ˆìœµ ì‚¬ì´íŠ¸ì—ì„œ í™•ì¸í•˜ì„¸ìš”.*\n"

    elif question_type == QUESTION_TYPE_PUBLIC_OPINION:
        answer = "**[íˆ¬ìì ì˜ê²¬ ìš”ì•½]**\n\n"

        if talks_data:
            answer += f"{talks_data.evidence_snippet}\n\n"
        else:
            answer += "ìµœê·¼ ì˜ê²¬ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"

        if price_data:
            answer += f"**ì°¸ê³  - í˜„ì¬ ì‹œì„¸:**\n{price_data.evidence_snippet}\n\n"

        answer += "*â€» ê°œì¸ ì˜ê²¬ì´ë¯€ë¡œ ì°¸ê³ ë§Œ í•˜ì‹œê³ , íˆ¬ì íŒë‹¨ì€ ë³¸ì¸ì˜ ì±…ì„í•˜ì— í•˜ì„¸ìš”.*\n"

    elif question_type == QUESTION_TYPE_NEWS_DISCLOSURE:
        answer = "**[ìµœê·¼ ë‰´ìŠ¤ ë° ê³µì‹œ]**\n\n"

        if news_data:
            answer += f"**ë‰´ìŠ¤:**\n{news_data.evidence_snippet}\n\n"

        found_disclosure = False
        for summary in summaries:
            if summary.source_type == "ê³µì‹œ":
                answer += f"**ê³µì‹œ:**\n{summary.evidence_snippet}\n\n"
                found_disclosure = True
                break

        if not news_data and not found_disclosure:
            answer += "ìµœê·¼ ë‰´ìŠ¤ë‚˜ ê³µì‹œë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"

    else:
        answer = "**[ì¢…ëª© ì •ë³´]**\n\n"

        if price_data:
            answer += f"{price_data.evidence_snippet}\n\n"
        else:
            answer += "ì¢…ëª© ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"

    return answer


def _generate_final_answer_llm(
    intent: IntentResult,
    summaries: List[SourceSummary],
    chat_history: List = None
) -> str:
    """
    Generate final answer using LLM (optional mode)
    Args:
        intent: Intent analysis result
        summaries: Source summaries
        chat_history: Previous chat messages for context (optional)
    Returns:
        Final answer text
    """
    try:
        # Check if OpenAI API key is available
        if not get_env('OPENAI_API_KEY'):
            import logging
            logger = logging.getLogger(__name__)
            logger.info("No OpenAI API key found, using basic template mode")
            return _generate_final_answer_basic(intent, summaries)

        # Prepare evidence snippets
        evidence = "\n\n".join([
            f"[{summary.source_type}]\n{summary.evidence_snippet}"
            for summary in summaries
        ])

        # Check if we have any evidence
        if not evidence.strip():
            import logging
            logger = logging.getLogger(__name__)
            logger.warning("No evidence data available for LLM")
            return _generate_final_answer_basic(intent, summaries)

        # Prepare chat history context
        history_context = ""
        if chat_history and len(chat_history) > 0:
            history_context = "\n**ì´ì „ ëŒ€í™” ë‚´ìš©:**\n"
            for msg in chat_history[-6:]:  # Last 3 exchanges
                # Handle both dict and object formats
                role_value = msg.get('role') if isinstance(msg, dict) else msg.role
                content_value = msg.get('content') if isinstance(msg, dict) else msg.content
                role = "ì‚¬ìš©ì" if role_value == "user" else "ì±—ë´‡"
                history_context += f"{role}: {content_value[:200]}...\n"
            history_context += "\n"

        prompt_text = f"""ë‹¹ì‹ ì€ **GPT-4o ê¸°ë°˜ ê¸ˆìœµ AI ì–´ì‹œìŠ¤í„´íŠ¸**ì…ë‹ˆë‹¤. ì‹¤ì‹œê°„ ì¸í„°ë„· ê²€ìƒ‰(Tavily API)ì„ í†µí•´ ë‹¤ìŒ ê¸ˆìœµì—ì„œ ìˆ˜ì§‘í•œ ìµœì‹  ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

{history_context}**í•µì‹¬ ì—­í• :**
1. âœ… **ì‹¤ì‹œê°„ ê²€ìƒ‰**: Tavily APIë¡œ ë‹¤ìŒ ê¸ˆìœµ(finance.daum.net)ì—ì„œ ìµœì‹  ì •ë³´ë¥¼ ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤
2. âœ… **ë°ì´í„° ë¶„ì„**: ì•„ë˜ ì œê³µëœ ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤
3. âœ… **ì´ˆë³´ì ì¹œí™”**: ë³µì¡í•œ ê¸ˆìœµ ì •ë³´ë¥¼ ì‰½ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤
4. âŒ **ì˜ˆì¸¡ ê¸ˆì§€**: ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•˜ê±°ë‚˜ íˆ¬ìë¥¼ ê¶Œìœ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤

**ê²€ìƒ‰ ëŒ€ìƒ ì¢…ëª©:**
- ì¢…ëª©ëª…: {intent.stock_name}
- ì¢…ëª© ì½”ë“œ: {intent.stock_code}
- ì§ˆë¬¸ ìœ í˜•: {intent.question_type}

**ğŸ“¡ ì‹¤ì‹œê°„ ê²€ìƒ‰ ê²°ê³¼ (Tavily API):**
{evidence}

---

**ë‹µë³€ ì‘ì„± ê°€ì´ë“œ:**

## ğŸ“Š {intent.stock_name} ({intent.stock_code}) ì‹¤ì‹œê°„ ë¶„ì„

### ğŸ’¡ í•œ ì¤„ ìš”ì•½
(ê²€ìƒ‰ëœ ë°ì´í„°ì˜ í•µì‹¬ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ)

### ğŸ“ˆ í˜„ì¬ ìƒí™© ë¶„ì„

**ì‹œì„¸ ì •ë³´** (ìˆëŠ” ê²½ìš°):
- í˜„ì¬ê°€, ë“±ë½ë¥ , ê±°ë˜ëŸ‰ ë“± êµ¬ì²´ì  ìˆ˜ì¹˜ ì œì‹œ
- ì „ì¼ ëŒ€ë¹„ ë³€ë™ ìƒí™©

**ìµœì‹  ë‰´ìŠ¤** (ìˆëŠ” ê²½ìš°):
- ì£¼ìš” ë‰´ìŠ¤ ì œëª©ê³¼ í•µì‹¬ ë‚´ìš©
- ë‰´ìŠ¤ê°€ ì£¼ê°€ì— ë¯¸ì¹  ìˆ˜ ìˆëŠ” ì˜í–¥

**ì‹œì¥ ë°˜ì‘** (ìˆëŠ” ê²½ìš°):
- íˆ¬ììë“¤ì˜ ì˜ê²¬ ë™í–¥
- í† ë¡ ë°© ì£¼ìš” ì˜ê²¬ ìš”ì•½

### âš ï¸ íˆ¬ì ì²´í¬í¬ì¸íŠ¸

âœ… **í™•ì¸ëœ ì •ë³´:**
- ìœ„ ì •ë³´ëŠ” ë‹¤ìŒ ê¸ˆìœµì—ì„œ ì‹¤ì‹œê°„ ê²€ìƒ‰í•œ ë°ì´í„°ì…ë‹ˆë‹¤
- [ë°ì´í„° ìˆ˜ì§‘ ì‹œê°„: ë°©ê¸ˆ ì „]

âš ï¸ **ì¶”ê°€ í™•ì¸ ì‚¬í•­:**
- ê¸°ì—… ì¬ë¬´ì œí‘œ (ë§¤ì¶œ, ì˜ì—…ì´ìµ, ë¶€ì±„ë¹„ìœ¨)
- ì—…ì¢… ì „ì²´ ë™í–¥ ë° ê²½ìŸì‚¬ ë¹„êµ
- ìµœê·¼ 3ê°œì›” ì°¨íŠ¸ ë¶„ì„

ğŸ’¬ **íˆ¬ì íŒë‹¨ ì‹œ ê³ ë ¤ì‚¬í•­:**
- ë³¸ì¸ì˜ íˆ¬ì ì„±í–¥ (ê³µê²©ì /ì•ˆì •ì )
- íˆ¬ì ê°€ëŠ¥ ê¸ˆì•¡ê³¼ ì†ì‹¤ ê°ë‚´ ë²”ìœ„
- ë¶„ì‚° íˆ¬ì ì›ì¹™ (í•œ ì¢…ëª© ì§‘ì¤‘ X)

---

**ì‘ì„± ê·œì¹™:**
âœ… **ì‚¬ìš© ê°€ëŠ¥í•œ í‘œí˜„:**
- "ê²€ìƒ‰ ê²°ê³¼ì— ë”°ë¥´ë©´ ~"
- "í˜„ì¬ ë°ì´í„° ê¸°ì¤€ ~"
- "ìµœì‹  ì •ë³´ë¡œëŠ” ~"
- "~ë¥¼ ì¶”ê°€ í™•ì¸í•˜ì‹œê¸¸ ê¶Œì¥í•©ë‹ˆë‹¤"

âŒ **ê¸ˆì§€ëœ í‘œí˜„:**
- "~í•  ê²ƒì…ë‹ˆë‹¤" (ë¯¸ë˜ ì˜ˆì¸¡)
- "ë§¤ìˆ˜/ë§¤ë„í•˜ì„¸ìš”" (íˆ¬ì ê¶Œìœ )
- "í™•ì‹¤í•©ë‹ˆë‹¤" (ë‹¨ì •ì  í‘œí˜„)
- ê²€ìƒ‰ë˜ì§€ ì•Šì€ ì •ë³´ ì–¸ê¸‰

ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:"""

        # Use OpenAI API
        from openai import OpenAI
        from config import LLM_MODEL_OPENAI, LLM_MAX_TOKENS, LLM_TEMPERATURE
        import logging
        logger = logging.getLogger(__name__)

        logger.info(f"Calling OpenAI API with model: {LLM_MODEL_OPENAI}")
        
        client = OpenAI(api_key=get_env('OPENAI_API_KEY'))

        response = client.chat.completions.create(
            model=LLM_MODEL_OPENAI,
            max_completion_tokens=LLM_MAX_TOKENS,
            temperature=LLM_TEMPERATURE,
            messages=[{"role": "user", "content": prompt_text}]
        )

        logger.info("OpenAI API call successful")
        return response.choices[0].message.content

    except Exception as e:
        import logging
        logger = logging.getLogger(__name__)
        logger.error(f"LLM answer generation failed: {str(e)}", exc_info=True)
        logger.info("Falling back to basic template mode")
        return _generate_final_answer_basic(intent, summaries)


def generate_answer(
    intent: IntentResult,
    plans: List[FetchPlan],
    summaries: List[SourceSummary],
    use_llm: bool = False,
    show_details: bool = True,
    chat_history: List = None
) -> str:
    """
    Generate 4-step structured answer

    Args:
        intent: Intent analysis result
        plans: Fetch plans
        summaries: Source summaries
        use_llm: Whether to use LLM for answer generation (default: False)
        show_details: Whether to show detailed steps 1-3 (default: True)
        chat_history: Previous chat messages for context (optional)

    Returns:
        Complete answer as markdown string
    """
    output = []

    # Show detailed steps only if requested
    if show_details:
        # Step 1: Intent Analysis
        output.append("### [1] ì§ˆë¬¸ ì˜ë„ ë¶„ì„\n")
        output.append(f"- **ì§ˆë¬¸ ìœ í˜•:** {intent.question_type}")
        output.append(f"- **ëŒ€ìƒ ì¢…ëª©:** {intent.stock_name or 'í™•ì¸ ë¶ˆê°€'} ({intent.stock_code or 'í™•ì¸ ë¶ˆê°€'})")
        output.append(f"- **ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ê²ƒ:** {_get_intent_description(intent.question_type)}\n")

        # Step 2: Exploration Plan
        output.append("### [2] ë‹¤ìŒ ê¸ˆìœµ íƒìƒ‰ ê³„íš\n")
        if plans:
            for i, plan in enumerate(plans, 1):
                output.append(f"- **Plan {i}:** {plan.description}")
                output.append(f"  - URL: `{plan.url}`")
        else:
            output.append("- íƒìƒ‰ ê³„íšì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ì¢…ëª© ì •ë³´ ë¶€ì¡±)\n")
        output.append("")

        # Step 3: Scraping Results Summary
        output.append("### [3] ë‹¤ìŒ ê¸ˆìœµ ìŠ¤í¬ë© ê²°ê³¼ ìš”ì•½\n")
        if summaries:
            for i, summary in enumerate(summaries, 1):
                output.append(f"**Source {i}: {summary.source_type}**")
                output.append(f"- URL: `{summary.source_url}`")
                output.append(f"- ê·¼ê±° ìŠ¤ë‹ˆí«:\n```\n{summary.evidence_snippet}\n```\n")
        else:
            output.append("- ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n")

        # Step 4: Final Answer
        output.append("### [4] ìµœì¢… ë‹µë³€ (ì´ˆë³´ì ì¹œí™”)\n")

    # Generate final answer (always shown)
    if summaries:
        if use_llm:
            final_answer = _generate_final_answer_llm(intent, summaries, chat_history)
        else:
            final_answer = _generate_final_answer_basic(intent, summaries)
        output.append(final_answer)
    else:
        output.append("ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ìˆëŠ” ì¶©ë¶„í•œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        output.append("ì¢…ëª© ì½”ë“œë¥¼ í™•ì¸í•˜ê±°ë‚˜, ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n")

    # Reference section - ALWAYS show (even when show_details=False)
    # This ensures users can verify all data comes from finance.daum.net
    output.append("\n---")
    
    if show_details:
        output.append("### ğŸ“ ì°¸ê³ í•œ ë‹¤ìŒ ê¸ˆìœµ í˜ì´ì§€\n")
    else:
        output.append("**ğŸ“ ì°¸ê³ í•œ ë‹¤ìŒ ê¸ˆìœµ í˜ì´ì§€**\n")

    if summaries:
        # Collect unique URLs
        reference_urls = []
        seen_urls = set()

        for summary in summaries:
            url = summary.source_url
            if url and url not in seen_urls:
                seen_urls.add(url)
                reference_urls.append({
                    'type': summary.source_type,
                    'url': url
                })

        # Display as clickable links
        if reference_urls:
            for i, ref in enumerate(reference_urls[:7], 1):  # Limit to 7 references
                # Extract a friendly name from URL or use source type
                friendly_name = ref['type'] or f"ì°¸ê³  {i}"
                output.append(f"{i}. [{friendly_name}]({ref['url']})")
        else:
            output.append("- ì°¸ê³  URL ì—†ìŒ")
    else:
        output.append("- ìˆ˜ì§‘ëœ ë°ì´í„° ì—†ìŒ")

    output.append("")

    # Footer - compact version
    if show_details:
        output.append("---")
        output.append("**âš ï¸ ì£¼ì˜ì‚¬í•­:**")
        output.append("- ë³¸ ì •ë³´ëŠ” ë‹¤ìŒ ê¸ˆìœµ(finance.daum.net) ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤")
        output.append("- íˆ¬ì íŒë‹¨ ë° ê²°ê³¼ì— ëŒ€í•œ ì±…ì„ì€ íˆ¬ìì ë³¸ì¸ì—ê²Œ ìˆìŠµë‹ˆë‹¤")
        output.append("- ì‹¤ì‹œê°„ ë°ì´í„°ê°€ ì•„ë‹ ìˆ˜ ìˆìœ¼ë‹ˆ, ì •í™•í•œ ì •ë³´ëŠ” ì§ì ‘ í™•ì¸í•˜ì„¸ìš”")
    else:
        # Minimal footer for clean mode
        output.append("\n\n---")
        output.append("*ë³¸ ì •ë³´ëŠ” ë‹¤ìŒ ê¸ˆìœµ ë°ì´í„° ê¸°ë°˜ì´ë©°, íˆ¬ì íŒë‹¨ì€ ë³¸ì¸ ì±…ì„ì…ë‹ˆë‹¤*")

    return "\n".join(output)
