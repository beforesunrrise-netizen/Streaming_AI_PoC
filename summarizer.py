"""
Scraping result summarization
Converts fetch results into evidence snippets
"""

from typing import List, Dict, Any, Optional
from dataclasses import dataclass

from daum_fetch import FetchResult
import parsers

# Daum Fetch imports (requests ê¸°ë°˜ - Streamlit Cloud í˜¸í™˜)
import daum_fetch
import endpoints


@dataclass
class SourceSummary:
    """
    Summary of a single data source
    """
    source_url: str
    source_type: str
    key_data: Dict[str, Any]
    evidence_snippet: str


def _summarize_price_data(data: Dict[str, Any]) -> str:
    """
    Create evidence snippet for price data
    Args:
        data: Parsed price data
    Returns:
        Evidence snippet string
    """
    if not data:
        return "ì‹œì„¸ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    parts = []

    # í˜„ì¬ê°€
    if 'current_price' in data and data['current_price']:
        price = data['current_price']
        if isinstance(price, (int, float)):
            parts.append(f"í˜„ì¬ê°€ {price:,}ì›")
        else:
            parts.append(f"í˜„ì¬ê°€ {price}")

    # ì „ì¼ëŒ€ë¹„ ë° ë“±ë½ë¥ 
    if 'change' in data and 'change_rate' in data:
        change = data['change']
        change_rate = data['change_rate']
        
        # ë“±ë½ ê¸°í˜¸
        if isinstance(change, (int, float)):
            if change > 0:
                parts.append(f"ì „ì¼ë¹„ â–²{change:,}ì› (+{change_rate}%)")
            elif change < 0:
                parts.append(f"ì „ì¼ë¹„ â–¼{abs(change):,}ì› ({change_rate}%)")
            else:
                parts.append(f"ì „ì¼ë¹„ ë³´í•© (0%)")
        else:
            parts.append(f"ì „ì¼ë¹„ {change} ({change_rate})")

    # ê±°ë˜ëŸ‰
    if 'volume' in data and data['volume']:
        volume = data['volume']
        if isinstance(volume, (int, float)):
            parts.append(f"ê±°ë˜ëŸ‰ {volume:,}ì£¼")
        else:
            parts.append(f"ê±°ë˜ëŸ‰ {volume}")

    # ì‹œê°€
    if 'open_price' in data and data['open_price']:
        open_price = data['open_price']
        if isinstance(open_price, (int, float)):
            parts.append(f"ì‹œê°€ {open_price:,}ì›")
        else:
            parts.append(f"ì‹œê°€ {open_price}")

    # ê³ ê°€/ì €ê°€
    if 'high_price' in data and 'low_price' in data:
        if data['high_price'] and data['low_price']:
            high = data['high_price']
            low = data['low_price']
            if isinstance(high, (int, float)) and isinstance(low, (int, float)):
                parts.append(f"ê³ ê°€ {high:,}ì›, ì €ê°€ {low:,}ì›")
            else:
                parts.append(f"ê³ ê°€ {high}, ì €ê°€ {low}")

    # ì‹œê°€ì´ì•¡ (pykrx ë°ì´í„°ì¸ ê²½ìš°)
    if 'market_cap' in data and data['market_cap']:
        market_cap = data['market_cap']
        if isinstance(market_cap, (int, float)):
            parts.append(f"ì‹œê°€ì´ì•¡ {market_cap:,}ì›")

    # ë°ì´í„° ì¶œì²˜ ì •ë³´
    if 'data_source' in data:
        source = data['data_source']
        if 'pykrx' in source.lower():
            parts.append("(í•œêµ­ê±°ë˜ì†Œ ê³µì‹ ë°ì´í„°)")
        elif source == 'chart_api':
            parts.append("(ì°¨íŠ¸ API ê¸°ì¤€)")
        else:
            parts.append(f"({source})")
    
    # ë‚ ì§œ ì •ë³´
    if 'data_date' in data and data['data_date']:
        parts.append(f"[ê¸°ì¤€: {data['data_date']}]")
    elif 'date' in data and data['date']:
        parts.append(f"[{data['date']}]")

    return ", ".join(parts) if parts else "ì‹œì„¸ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."


def _summarize_news_data(news_list: List[Dict[str, str]]) -> str:
    """
    Create evidence snippet for news data
    Args:
        news_list: List of news items
    Returns:
        Evidence snippet string
    """
    if not news_list:
        return "ìµœê·¼ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."

    # Take top 5 news
    top_news = news_list[:5]
    snippets = []

    for i, news in enumerate(top_news, 1):
        title = news.get('title', 'ì œëª© ì—†ìŒ')
        date = news.get('date', '')
        summary = news.get('summary', '')
        
        # Build snippet with title
        snippet = f"{i}. **{title}**"
        if date:
            snippet += f" ({date})"
        
        # Add summary if available (helps LLM understand context)
        if summary:
            snippet += f"\n   > {summary}"
        
        snippets.append(snippet)

    return "\n".join(snippets)


def _summarize_talks_data(talks_list: List[Dict[str, str]]) -> str:
    """
    Create evidence snippet for talks/opinion data
    Args:
        talks_list: List of talk items
    Returns:
        Evidence snippet string
    """
    if not talks_list:
        return "ìµœê·¼ ì˜ê²¬ì´ ì—†ìŠµë‹ˆë‹¤."

    # Take top 5 talks
    top_talks = talks_list[:5]
    snippets = []

    for i, talk in enumerate(top_talks, 1):
        content = talk.get('content', 'ë‚´ìš© ì—†ìŒ')[:100]  # Limit length
        date = talk.get('date', '')
        snippet = f"{i}. {content}"
        if date:
            snippet += f" ({date})"
        snippets.append(snippet)

    return "\n".join(snippets)


def _summarize_disclosure_data(disc_list: List[Dict[str, str]]) -> str:
    """
    Create evidence snippet for disclosure data
    Args:
        disc_list: List of disclosure items
    Returns:
        Evidence snippet string
    """
    if not disc_list:
        return "ìµœê·¼ ê³µì‹œê°€ ì—†ìŠµë‹ˆë‹¤."

    # Take top 5 disclosures
    top_disc = disc_list[:5]
    snippets = []

    for i, disc in enumerate(top_disc, 1):
        title = disc.get('title', 'ì œëª© ì—†ìŒ')
        date = disc.get('date', '')
        disc_type = disc.get('type', '')
        snippet = f"{i}. [{disc_type}] {title}"
        if date:
            snippet += f" ({date})"
        snippets.append(snippet)

    return "\n".join(snippets)


def _summarize_chart_data(chart_list: List[Dict[str, Any]]) -> str:
    """
    Create evidence snippet for chart data
    Args:
        chart_list: List of chart data points
    Returns:
        Evidence snippet string
    """
    if not chart_list:
        return "ì°¨íŠ¸ ë°ì´í„°ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # Take last 5 days
    recent = chart_list[-5:] if len(chart_list) >= 5 else chart_list

    if len(recent) < 2:
        return "ì¶©ë¶„í•œ ì°¨íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

    # Calculate trend
    first_close = recent[0].get('close', 0)
    last_close = recent[-1].get('close', 0)

    if first_close > 0:
        change_pct = ((last_close - first_close) / first_close) * 100
        trend = "ìƒìŠ¹" if change_pct > 0 else "í•˜ë½"
        snippet = f"ìµœê·¼ {len(recent)}ì¼ê°„ {trend} ì¶”ì„¸ ({change_pct:+.2f}%)"
    else:
        snippet = "ì¶”ì„¸ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # Add latest data
    latest = recent[-1]
    snippet += f"\nìµœê·¼ ì¢…ê°€: {latest.get('close', 0):,}ì›"

    return snippet


def get_realtime_stock_summary_from_daum(stock_code: str) -> Optional[SourceSummary]:
    """
    ë‹¤ìŒ ê¸ˆìœµì—ì„œ ì‹¤ì‹œê°„ ì£¼ì‹ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ SourceSummaryë¡œ ë³€í™˜
    requestsë¥¼ ì‚¬ìš©í•˜ì—¬ API/HTML íŒŒì‹± (Streamlit Cloud í˜¸í™˜)
    
    Args:
        stock_code: ì¢…ëª© ì½”ë“œ (ì˜ˆ: "005930")
    
    Returns:
        SourceSummary ê°ì²´ ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
    """
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        # 1. Finance API ì‹œë„ (ê°€ì¥ ì•ˆì •ì , í…ŒìŠ¤íŠ¸ ì™„ë£Œ)
        api_url = endpoints.get_finance_api_url(stock_code)
        result = daum_fetch.fetch(api_url, is_json=True, use_cache=False)
        
        data = None
        if result.success and result.json_data:
            data = parsers.parse_api_quote(result.json_data)
            if data:
                logger.info("âœ… Finance APIë¡œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì„±ê³µ")
        
        # 2. Chart API ì‹œë„ (í´ë°±)
        if not data:
            chart_url = endpoints.get_chart_api_url(stock_code, "days")
            result = daum_fetch.fetch(chart_url, is_json=True, use_cache=False)
            
            if result.success and result.json_data:
                data = parsers.parse_chart_for_price(result.json_data)
                if data:
                    logger.info("âœ… Chart APIë¡œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì„±ê³µ")
        
        # 3. HTML í˜ì´ì§€ íŒŒì‹± ì‹œë„ (ìµœí›„ ìˆ˜ë‹¨)
        if not data:
            price_url = endpoints.get_price_url(stock_code)
            result = daum_fetch.fetch(price_url, use_cache=False)
            
            if result.success:
                data = parsers.parse_price_page(result.content)
                if data:
                    logger.info("âœ… HTML íŒŒì‹±ìœ¼ë¡œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì„±ê³µ")
        
        if not data:
            logger.warning(f"Failed to get stock data from Daum for {stock_code}")
            return None
        
        snippet = _summarize_price_data(data)
        
        return SourceSummary(
            source_url=f"https://finance.daum.net/quotes/A{stock_code}",
            source_type="ì‹¤ì‹œê°„ ì‹œì„¸ (ë‹¤ìŒ ê¸ˆìœµ)",
            key_data=data,
            evidence_snippet=snippet
        )
    except Exception as e:
        logger.error(f"Failed to get stock data from Daum Finance: {str(e)}")
        return None


def get_realtime_stock_summary(stock_code: str) -> Optional[SourceSummary]:
    """
    ë‹¤ìŒ ê¸ˆìœµì—ì„œ ì‹¤ì‹œê°„ ì£¼ì‹ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ SourceSummaryë¡œ ë³€í™˜
    
    âš ï¸ ì¤‘ìš”: ë°˜ë“œì‹œ ë‹¤ìŒ ê¸ˆìœµ(finance.daum.net)ì—ì„œë§Œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Args:
        stock_code: ì¢…ëª© ì½”ë“œ (ì˜ˆ: "005930")
    
    Returns:
        SourceSummary ê°ì²´ ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
    """
    import logging
    logger = logging.getLogger(__name__)
    
    # ë‹¤ìŒ ê¸ˆìœµ (Selenium)ë§Œ ì‚¬ìš©
    daum_result = get_realtime_stock_summary_from_daum(stock_code)
    
    if daum_result:
        logger.info("âœ… Successfully got data from Daum Finance (finance.daum.net)")
        return daum_result
    else:
        logger.warning("âŒ Failed to get data from Daum Finance")
        return None


def get_talks_summary_from_daum(stock_code: str, stock_name: str = None) -> Optional[SourceSummary]:
    """
    Tavilyë¥¼ ì‚¬ìš©í•˜ì—¬ ì¢…ëª© ê´€ë ¨ íˆ¬ìì ì˜ê²¬/ë¶„ì„ì„ ê²€ìƒ‰
    ë‹¤ìŒ ê¸ˆìœµ í† ë¡  í˜ì´ì§€ê°€ 404ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ Tavilyë¡œ ëŒ€ì²´
    
    Args:
        stock_code: ì¢…ëª© ì½”ë“œ
        stock_name: ì¢…ëª©ëª… (optional)
    
    Returns:
        SourceSummary ê°ì²´ ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
    """
    import logging
    import os
    logger = logging.getLogger(__name__)
    
    try:
        from tavily import TavilyClient
        
        # Tavily API í‚¤ í™•ì¸
        tavily_api_key = os.getenv('TAVILY_API_KEY')
        if not tavily_api_key:
            logger.warning("Tavily API key not found, skipping investor opinions search")
            return None
        
        client = TavilyClient(api_key=tavily_api_key)
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        search_query = f"{stock_name or stock_code} ì¢…ëª© íˆ¬ì ì˜ê²¬ ë¶„ì„ ì „ë§"
        
        # Tavily ê²€ìƒ‰
        response = client.search(
            query=search_query,
            search_depth="basic",
            max_results=3
        )
        
        if not response or 'results' not in response or len(response['results']) == 0:
            logger.warning(f"No investor opinions found via Tavily for {stock_code}")
            return None
        
        # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
        snippets = []
        for i, result in enumerate(response['results'][:3], 1):
            title = result.get('title', 'ì œëª© ì—†ìŒ')
            content = result.get('content', '')[:150]
            
            snippet_line = f"{i}. {title}"
            if content:
                snippet_line += f"\n   {content}..."
            snippets.append(snippet_line)
        
        snippet = "\n\n".join(snippets)
        
        logger.info(f"âœ… Tavilyë¡œ {len(response['results'])}ê°œì˜ íˆ¬ì ì˜ê²¬ ê²€ìƒ‰ ì™„ë£Œ")
        
        return SourceSummary(
            source_url="Tavily ê²€ìƒ‰ ê²°ê³¼",
            source_type="íˆ¬ìì ì˜ê²¬ ë° ë¶„ì„",
            key_data={'results': response['results'][:3]},
            evidence_snippet=f"ğŸ’¬ **íˆ¬ìì ì˜ê²¬ ë° ë¶„ì„:**\n\n{snippet}"
        )
    except ImportError:
        logger.warning("Tavily not installed, skipping investor opinions search")
        return None
    except Exception as e:
        logger.error(f"Failed to search investor opinions via Tavily: {str(e)}")
        return None


def summarize_results(
    fetch_results: List[tuple],
    plans: List,
    stock_code: Optional[str] = None,
    stock_name: Optional[str] = None,
    include_realtime: bool = True
) -> List[SourceSummary]:
    """
    Summarize all fetch results into evidence snippets
    Only includes successful fetches with valid data

    Args:
        fetch_results: List of (FetchResult, FetchPlan) tuples
        plans: List of FetchPlan objects (for reference)
        stock_code: ì¢…ëª© ì½”ë“œ (ì‹¤ì‹œê°„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°ìš©, optional)
        stock_name: ì¢…ëª©ëª… (optional, Tavily ê²€ìƒ‰ì— ì‚¬ìš©)
        include_realtime: ì‹¤ì‹œê°„ ë°ì´í„° í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸: True)

    Returns:
        List of SourceSummary objects (only successful ones)
    """
    import logging
    logger = logging.getLogger(__name__)
    summaries = []
    
    # 1. ë‹¤ìŒ ê¸ˆìœµì—ì„œ ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ (requests + API)
    if include_realtime and stock_code:
        logger.info(f"ğŸ“Š Fetching stock data from Daum Finance for {stock_code}")
        
        # ì‹œì„¸ ë°ì´í„°
        realtime_summary = get_realtime_stock_summary(stock_code)
        if realtime_summary:
            summaries.append(realtime_summary)
            logger.info(f"âœ… Added stock price data from finance.daum.net")
        
        # íˆ¬ìì ì˜ê²¬ (Tavily ê²€ìƒ‰)
        logger.info(f"ğŸ’¬ Searching investor opinions via Tavily for {stock_code}")
        talks_summary = get_talks_summary_from_daum(stock_code, stock_name)
        if talks_summary:
            summaries.append(talks_summary)
            logger.info(f"âœ… Added investor opinions via Tavily search")

    # 2. ê¸°ì¡´ Daum Finance ë°ì´í„° ì²˜ë¦¬
    for fetch_result, plan in fetch_results:
        # Special handling for Tavily news - use pre-fetched content OR fetch actual page
        if plan.parser_name == "tavily_news":
            try:
                # Use Tavily's content if available and substantial
                content_text = plan.content if hasattr(plan, 'content') and plan.content else ""
                
                # If Tavily content is too short or empty, try fetching the actual page
                if len(content_text.strip()) < 100:
                    logger.info(f"Tavily content too short ({len(content_text)} chars), fetching actual page: {plan.url}")
                    
                    # Try to fetch and parse the news page
                    from daum_fetch import fetch
                    
                    # Only fetch if it's a news list page (not individual article)
                    if '/news' in plan.url and not any(x in plan.url for x in ['/stock/', '/economy/', '/industry/', '/world/']):
                        fetch_result_actual = fetch(plan.url, use_cache=True)
                        
                        if fetch_result_actual.success and fetch_result_actual.content:
                            # Try to parse news list
                            news_list = parsers.parse_news_list(fetch_result_actual.content)
                            
                            if news_list and len(news_list) > 0:
                                # Use top 3 news from the page
                                news_snippets = []
                                for i, news_item in enumerate(news_list[:3], 1):
                                    title = news_item.get('title', '')
                                    date = news_item.get('date', '')
                                    snippet_text = f"{i}. **{title}**"
                                    if date:
                                        snippet_text += f" ({date})"
                                    news_snippets.append(snippet_text)
                                
                                snippet = "ğŸ“° **ìµœì‹  ë‰´ìŠ¤:**\n" + "\n".join(news_snippets)
                                parsed_data = {
                                    "title": plan.title or "ë‰´ìŠ¤ ëª©ë¡",
                                    "url": plan.url,
                                    "news_list": news_list[:3]
                                }
                                
                                summaries.append(SourceSummary(
                                    source_url=plan.url,
                                    source_type="ë‰´ìŠ¤",
                                    key_data=parsed_data,
                                    evidence_snippet=snippet
                                ))
                                logger.info(f"Successfully fetched and parsed {len(news_list[:3])} news items from {plan.url}")
                                continue
                
                # Use Tavily content if available
                if plan.title and content_text:
                    # Extract first 300 chars of content for snippet
                    content_preview = content_text[:300].replace('\n', ' ').strip()
                    if len(content_preview) > 0:
                        snippet = f"ğŸ“° **{plan.title}**\n\n{content_preview}..."
                    else:
                        snippet = f"ğŸ“° **{plan.title}**"
                elif plan.title:
                    snippet = f"ğŸ“° **{plan.title}**"
                else:
                    # Skip if no title and no content
                    logger.warning(f"Skipping Tavily news with no title and no content: {plan.url}")
                    continue
                
                parsed_data = {
                    "title": plan.title,
                    "url": plan.url,
                    "content": content_text[:1000] if content_text else ""  # First 1000 chars for LLM
                }
                source_type = "ë‰´ìŠ¤"

                summaries.append(SourceSummary(
                    source_url=plan.url,
                    source_type=source_type,
                    key_data=parsed_data,
                    evidence_snippet=snippet
                ))
                logger.info(f"Added Tavily news: {plan.title} ({len(content_text)} chars)")
            except Exception as e:
                logger.error(f"Error processing Tavily news: {str(e)}", exc_info=True)
                pass  # Skip on error
            continue  # Move to next result

        if not fetch_result.success:
            # Skip failed fetches instead of adding them
            # This allows graceful degradation
            continue

        # Parse based on parser_name
        parser_name = plan.parser_name
        parsed_data = None

        try:
            if parser_name == "parse_price_page":
                parsed_data = parsers.parse_price_page(fetch_result.content or "")
                snippet = _summarize_price_data(parsed_data)
                source_type = "ì‹œì„¸ ì •ë³´"

            elif parser_name == "parse_chart_for_price":
                parsed_data = parsers.parse_chart_for_price(fetch_result.json_data or {})
                snippet = _summarize_price_data(parsed_data)
                source_type = "ì‹œì„¸ ì •ë³´ (ì°¨íŠ¸ API)"

            elif parser_name == "parse_api_quote":
                parsed_data = parsers.parse_api_quote(fetch_result.json_data or {})
                snippet = _summarize_price_data(parsed_data)
                source_type = "ì‹œì„¸ ì •ë³´ (API)"

            elif parser_name == "parse_news_list":
                parsed_data = parsers.parse_news_list(fetch_result.content or "")
                snippet = _summarize_news_data(parsed_data)
                source_type = "ë‰´ìŠ¤"

            elif parser_name == "parse_talks_list":
                parsed_data = parsers.parse_talks_list(fetch_result.content or "")
                snippet = _summarize_talks_data(parsed_data)
                source_type = "í† ë¡ /ì˜ê²¬"

            elif parser_name == "parse_disclosure_list":
                parsed_data = parsers.parse_disclosure_list(fetch_result.content or "")
                snippet = _summarize_disclosure_data(parsed_data)
                source_type = "ê³µì‹œ"

            elif parser_name == "parse_chart_json":
                parsed_data = parsers.parse_chart_json(fetch_result.json_data or {})
                snippet = _summarize_chart_data(parsed_data)
                source_type = "ì°¨íŠ¸"

            else:
                parsed_data = {}
                snippet = "ì•Œ ìˆ˜ ì—†ëŠ” ë°ì´í„° í˜•ì‹ì…ë‹ˆë‹¤."
                source_type = plan.description

            # Only add if we got valid data
            if parsed_data or snippet != "ì‹œì„¸ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.":
                summaries.append(SourceSummary(
                    source_url=fetch_result.url or plan.url,
                    source_type=source_type,
                    key_data=parsed_data if isinstance(parsed_data, dict) else {"data": parsed_data},
                    evidence_snippet=snippet
                ))

        except Exception as e:
            # Skip parsing errors instead of adding them
            # This allows graceful degradation
            continue

    return summaries
